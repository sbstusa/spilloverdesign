---
title: Notes on a Randomized Saturation Design
author: Jake Bowers
date: '`r format(Sys.Date(), "%B %d, %Y")`'
bibliography: bibliography.bib
published: true
graphics: yes
fontsize: 10pt
geometry: margin=1in
mainfont: "Minion Pro"
output:
  html_document:
    graphics: yes
    fig_caption: true
    fig_height: 4
    fig_width: 4
    keep_md: true
    toc: true
    highlight: pygments
  pdf_document:
    latex_engine: xelatex
    graphics: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    toc: true
  word_document:
    fig_height: 3
    fig_width: 5
---


```{r include=FALSE, cache=FALSE}
###  Make sure that you have run the runfirst-setup.R file
### What would be a good test for this that would stop the running of this file?

# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

## To make the html file within the R console do
## render("saturationDesign.Rmd",output_format=html_document(fig_retina=FALSE))
## To make the pdf file do
## render("saturationDesign.Rmd",output_format=pdf_document())
## Or at the unix command line: Rscript -e "library(rmarkdown); render('saturationDesign.Rmd')"

require(knitr)
opts_chunk$set(
	       tidy=FALSE,     # display code as typed
	       size="small",    # slightly smaller font for code
	       echo=TRUE,
	       results='markup',
	       strip.white=TRUE,
	       cache=FALSE,
	       highlight=TRUE,
	       width.cutoff=132,
	       size='footnotesize',
	       out.width='.9\\textwidth',
	       message=FALSE,
	       comment=NA)
```



A randomized saturation design is like the simple two-level design that we describe in [twolevelrand.Rmd](twolevelrand.Rmd) but involves random assignment of proportion treated to counties, or random assignment of "saturation" of the treatment.^[We follow @baird2015designing, who in turn build on @sinclair2012detecting, among others.]  For example, we might randomly assign counties to receive between 0% treated (i.e. the "pure control" counties in the two-level design) and 100% treated, with some saturations in between (like 33%, 50%, 66%, etc.).

This design raises a couple of new questions for us.  If our total number of treatments is fixed at $n_t$, then, to maximize power and enhance interpretibility: How many saturation categories should we use? How many counties should we choose?

# Blocks

## State
We will block first on state because the agricultural outcomes and farmer behavior will vary by state. We expect, that the relationship between microloans applications, spillover, and other outcomes, might also vary by state (in part as a proxy for ecosystem/type of agriculture and in part as a proxy for other influences that states have on agricultural production). Also, since counties tend to have the same land area size within a state, but vary greatly in land area across states, we expect different patterns in spillover by state --- assigning treatment to 50% of the farmers in one large county might incur different spillover than 50% of the farmers in a smaller county.

## County Size (number of farmers)
If we are randomly assigning counties to saturation, we should also probably should assign saturation to sets of counties that are similar in number of farmers within them. Across many counties, the distribution of county size should be balanced across saturation treatments, in expectation, but in our finite sample, we think that we will enhance power and make comparisons easier to understand if, within states, we randomly assign saturation within blocks of similar counties (with number of farmers within the county being a key covariate, but perhaps there are others that matter as well).

## Other county characteristics that moderate treatment and/or predict outcomes?

# Saturation

Within blocks, we know that we want at least one county at saturation 0% (i.e the "pure control") condition.

# Power

We have two main effects: the ITT (whereby we compare treated to pure control individuals), the Spillover on the Non-Treated (SNT) (whereby we compare non-treated individuals in a county with some saturation $\pi>0$ to pure control individuals (people in counties with $\pi=0$).

The power of the design will depend on overall experimental pool $N$ (roughly 3 million); number treated $n_t$; the variation in the outcome, $Y$; the size of the direct effect of treatment; the size of the spillover effect; the intra-county dependence (measured by the intra-cluster correlation coefficient (ICC)); and the correlation between treatment status within counties. The Baird et al 2015 piece provides

## A Note on Inferential Frameworks

The Baird et al 2015 piece uses a random effects/model-based mode for statistical inference. They claim that this very much simplifies the work required for power analysis. Although, in general, I tend to advocate randomization inference for randomized experiments, I suspect that we might gain from following their lead here, especially since our total pool of units is so large --- that the concerns about consistency and the CLT that one sidesteps with randomization infeerence in its permutation form might not be that important. That said, we might still want to check our results using a more direct randomization inference approach if we have few saturation categories.


# What do our data look like?

The following reads the data from Google Drive, assuming that there is a Google Sheet called "county-counts.csv" in your sheets directory.

```{r getdatafromgoogle}
library(googlesheets)
countyCountsGS<-gs_title("countyCountsFIPS.csv") ## get info about the data
countyCounts<-as.data.frame(gs_read_csv(countyCountsGS,as.is=TRUE),stringsAsFactors=FALSE)
## countyCounts<-countyCounts[countyCounts$state!="PR",]
str(countyCounts)
countyCounts$STATEFIPS<-with(countyCounts,ifelse(state.fips<10,paste("0",state.fips,sep=""),as.character(state.fips)))
countyCounts$COUNTYNAME<-countyCounts$county
countyCounts$COUNTYFIPS<-with(countyCounts,ifelse(county.fips<10,
						  paste("00",county.fips,sep=""),
						  ifelse(county.fips<100,paste("0",county.fips,sep=""),
							 as.character(county.fips))))

countyCounts$combifips<-with(countyCounts,paste(STATEFIPS,COUNTYFIPS,sep=""))
str(countyCounts)
```

```{r descusdadat}
sapply(countyCounts,function(x){ sum(is.na(x) | x=="" ) })
with(countyCounts,tapply(n,state,sum)) ## N by state
with(countyCounts,tapply(county,state,function(x){ length(unique(x) ) })) ## number of counties by state
summary(with(countyCounts,tapply(n,county,sum))) ## number of farmers by county
sapply(split(countyCounts$n,countyCounts$state),function(x){ summary(x) }) ## dist of county sizes by state
```

Remove counties with fewer than 2 farmers (these seem to be duplicates in the file anyway):

```{r}
countyCounts<-countyCounts[countyCounts$n>2,]
names(table(countyCounts$combifips))[table(countyCounts$combifips)>1]
```

Anchorage is still duplicated, so removing by hand. Eventually the input file should only have one row per county.

```{r}
countyCounts<-countyCounts[!(countyCounts$n==4 & countyCounts$combifips=="02020"),]
stopifnot(all(table(countyCounts$combifips)==1))
```


#  County to County Distances

We can have three measures of distance between counties: euclidean distance
between geographic centers, euclidean distance between centers of population,
and an indicator (1 or 0) for whether two counties are adjacent to each other.^[
Some of the sources:
Centers of population <http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt>
<https://www.census.gov/geo/reference/centersofpop.html>
County Adjacency:
<https://www.census.gov/geo/reference/county-adjacency.html>
]

## County Adjacency

This file required a bit of reformatting work. The file that does the cleanup is `gis/setupAdjData.sh`. It should be run once at the command line. The countyAdjClean.txt file will be in the repository. These lines just document the contents of `setupAdjData.sh`.

```{r engine = 'bash', eval = FALSE}
##These are bash commands.
## They are currently in gis/setupAdjData.sh which can be run at the command line.
curl -O http://www2.census.gov/geo/docs/reference/county_adjacency.txt
cp county_adjacency.txt tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/"\t/";\t/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t\t"/\t\t;;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t"/\t;"/g' tmp1.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/\t//g' tmp1.txt
## Add "Watonwan County, MN" to line 9625
LC_CTYPE=C LANG=C sed -i.bak '9629s/.*/"Watonwan County, MN";27165;"Blue Earth County, MN";27013/' tmp1.txt
## Change the old FIPS code of 02195 for Petersburgh Alaska to the new one of 02280
LC_CTYPE=C LANG=C sed -i.bak 's/02195/02280/g' tmp1.txt
LC_CTYPE=C LANG=C tr  '\n' ':'  < tmp1.txt > tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:;;/;/g'  tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/:/\\\n/g'  tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ County//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ Census Area//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ and Borough//g' tmp2.txt
LC_CTYPE=C LANG=C sed -i.bak $'s/ Borough//g' tmp2.txt
cp tmp2.txt countyAdjClean.txt
```


```{r readadjmat}
## https://stackoverflow.com/questions/5411979/state-name-to-abbreviation-in-r

Sys.setlocale('LC_ALL','C')

cntyAdjList1<-scan("gis/countyAdjClean.txt",what="",sep="\n",quote='\"')
cntyAdjList2<-strsplit(cntyAdjList1,";")
centerCounties1<-sapply(cntyAdjList2,function(obj){obj[1]})
centerCountiesFIPS1<-sapply(cntyAdjList2,function(obj){obj[2]})
centerCounties1[centerCounties1=="Prince of Wales-Hyder Census Area"]<-"Prince Wales Hyder"
names(cntyAdjList2)<-centerCounties1

## Remove Puerto Rico
centerCounties2<-centerCounties1[grep("PR$|VI$|GU$|AS$|MP$",centerCounties1,invert=TRUE)]
centerCountiesFIPS2<-centerCountiesFIPS1[grep("PR$|VI$|GU$|AS$|MP$",centerCounties1,invert=TRUE)]
cntyAdjList3<-cntyAdjList2[centerCounties2]

adjacentCounties<-lapply(cntyAdjList3,function(x){
			   if(length(x)<5){
			     return("None")
			   } else {
			     grep(",",x[3:length(x)],value=TRUE)
}})

isolatedCounties<-sapply(adjacentCounties,function(x){ all(x=="None") })
cntyAdjList3[isolatedCounties]

adjacentCountiesFIPS<-lapply(cntyAdjList3,function(x){ grep('[0-9]$',x[3:length(x)],value=TRUE) })
names(adjacentCountiesFIPS)<-centerCountiesFIPS2

```



## Centroids based on geography and based on population density.

```{r setupcensusfiles}
states<-read.csv("gis/StateNamesAndAbbreviations.csv",as.is=TRUE)

usaCountiesPopCentroids<-read.csv(url("http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt"),as.is=TRUE)
usaCountiesPopCentroids$state<-states$Postal.Code[match(usaCountiesPopCentroids$STNAME,states$State)]
usaCountiesPopCentroids$STATEFIPS<-with(usaCountiesPopCentroids,ifelse(STATEFP<10,paste("0",STATEFP,sep=""),as.character(STATEFP)))
usaCountiesPopCentroids$COUNTYNAME<-usaCountiesPopCentroids$COUNAME

str(usaCountiesPopCentroids)
usaCountiesPopCentroids$COUNTYFIPS<-with(usaCountiesPopCentroids,ifelse(COUNTYFP<10,
									paste("00",COUNTYFP,sep=""),
									ifelse(COUNTYFP<100,paste("0",COUNTYFP,sep=""),
									       as.character(COUNTYFP))))

library(foreign)
usaCountiesGeoCentroids<-read.dbf("gis/AddCombiFips/USACountyCentroids.dbf",as.is=TRUE)

tmp<-t(sapply(split(usaCountiesGeoCentroids,usaCountiesGeoCentroids$COMBIFIPS),function(dat){
		sapply(dat,function(x){
			 if(is.character(x)){
			   return(unique(x))
			 } else {
			   return(mean(x))
									}})}))

usaCountiesGeoCentroidsAvg<-as.data.frame(tmp,stringsAsFactors=FALSE)
usaCountiesGeoCentroidsAvg$CENTRDLON<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLON)
usaCountiesGeoCentroidsAvg$CENTRDLAT<-as.numeric(usaCountiesGeoCentroidsAvg$CENTRDLAT)
str(usaCountiesGeoCentroidsAvg)

## Dumping repeated counties --- basically, multi-island counties with multiple geographic centroids
censusCounties<-merge(usaCountiesPopCentroids,usaCountiesGeoCentroidsAvg,all.x=TRUE,all.y=FALSE,by=c("COUNTYFIPS","STATEFIPS"))

## Clifton Forge, VA is a town not a county as of 2010, but only in usaCountiesGeo so not in this file
## censusCounties<-censusCounties[censusCounties$COUNTYNAME.x!="Clifton Forge",]

summary(censusCounties)

## There are a few places with population centroids but no geographic centroids. This is ok. I think that we'd prefer population centroids anyway.
blah<-censusCounties[is.na(censusCounties$CENTRDLAT),"COUNTYNAME.x"]
countyCounts$county[countyCounts$county %in% blah]
grep("Prince",countyCounts$county,value=TRUE)
censusCounties$COUNTYNAME.x[censusCounties$COUNTYNAME.x=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"

## Remove Puerto Rico
censusCounties<-censusCounties[!(censusCounties$STNAME=="Puerto Rico" | censusCounties$state=="PR" | censusCounties$STATEFIPS=="72"),]
stopifnot(all(is.na(unlist(censusCounties[is.na(censusCounties$state),]))))
censusCounties<-censusCounties[!is.na(censusCounties$state),]

## Merge onto existing data.

censusCounties$county<-censusCounties$COUNTYNAME.x
usaCountiesPopCentroids$COUNTYNAME[usaCountiesPopCentroids$COUNTYNAME=="Prince of Wales-Hyder"]<-"Prince Wales Hyder"
usaCountiesPopCentroids$county<-usaCountiesPopCentroids$COUNTYNAME

tmpdat<-merge(countyCounts,censusCounties,by=c("county","state"),all.x=TRUE,all.y=FALSE)
##tmpdat<-merge(countyCounts,usaCountiesPopCentroids,by=c("county","state"),all.x=TRUE,all.y=FALSE)
tmpdat$ids<-paste(tmpdat$county,tmpdat$state,sep=", ")


## We have duplicated records because, I think, some citys and counties are both cities and counties.
## https://en.wikipedia.org/wiki/List_of_counties_in_Virginia
with(tmpdat,table(ids)[table(ids)!=1])
blah<-names(with(tmpdat,table(ids)[table(ids)!=1]))
blahdat<-tmpdat[tmpdat$ids %in% blah,]
blahdat<-blahdat[order(blahdat$county,blahdat$POPULATION,decreasing=TRUE),]
## The county FIPS codes are (from the TR-65 FIPS Code Chart)

bad<-with(tmpdat,{(county=="Richmond"  & state=="VA" & COUNTYFP!="159")  |
	  (county=="Bedford"   & state=="VA" & COUNTYFP!="019") |
	  (county=="Fairfax"   & state=="VA" & COUNTYFP!="059") |
	  (county=="Roanoke"   & state=="VA" & COUNTYFP!="161")|
	  (county=="Franklin"  & state=="VA" & COUNTYFP!="067") |
	  (county=="Baltimore" & state=="MD" & COUNTYFP!="005")})

tmpdat<-tmpdat[!bad,]
stopifnot(all(table(tmpdat$ids)==1))
row.names(tmpdat)<-tmpdat$ids

```

# Find pairs


Remove counties with only 1 farmer
```{r}
tmpdat<-tmpdat[tmpdat$n>1,]
```

Remove counties with no geographic information:
```{r}
tmpdat<-tmpdat[!is.na(tmpdat$LATITUDE),]
```

Also trim the top and bottom 1 percent of counties (in terms of size) within each county.

```{r}
stateCountyFarmDist<-tapply(tmpdat$n,tmpdat$state,function(x){ quantile(x,c(0,.01,.1,.25,.5,.75,.9,.99,1)) })
table(tmpdat$state)

datList<-lapply(split(tmpdat,tmpdat$state),function(dat){
		  qs<-quantile(dat$n,c(.01,.99))
		  return(dat[dat$n >= qs[[1]] & dat$n <= qs[[2]],])
	  })

sapply(datList,nrow)
sum(sapply(datList,nrow))

wrkdat<-do.call("rbind",datList)
## Exclude delaware, rhode island, and hawaii as having too few counties and people.
wrkdat<-wrkdat[!(wrkdat$state %in% c("DE","RI","HI")),]

row.names(wrkdat)<-wrkdat$combifips

```

## Find Pairs

The input to the pairing function is a matrix of distances. So first we make distance matrices.

```{r setuppairs}
library(nbpMatching)
```

Make an adjacency matrix of the counties:

```{r adjmat}
## Make a matrix with 1 indicating that two counties are touching and 0 otherwise.
#### Not all counties in the adjacency matrix are represented in the working data from the USDA
#### blah<-sapply(centerCountiesFIPS2,function(x){
#### 	       all(adjacentCountiesFIPS[[x]] %in% wrkdat$combifips ) })
#### table(blah)

adjList<-lapply(wrkdat$combifips,function(x){
		  wrkdat$combifips %in% adjacentCountiesFIPS[[x]]
	  })
names(adjList)<-wrkdat$combifips

adjMat<-do.call("rbind",adjList)
stopifnot(all(diag(adjMat))) ## all diagonals should be true
adjMat<-adjMat*1 # convert to numbers
dimnames(adjMat)<-list(wrkdat$combifips,wrkdat$combifips)
diag(adjMat)<-0 ## set self-distances to zero for ease in penalty creation below
wrkdat[which(rowMeans(adjMat)==0 ),] ## which counties have no adjacent counties (Honolulu and Maui in Hawaii)


## Some counties in Minnesota are causing a problem with the adjacency matrix which should be symmetric
## That is, for example, county 27005 is counted as being adjacent to county 27153 but 27153 is not recorded as being adjacent to county 27005. Since adjacency is a symmetric property (by definition here) I need to fix this.
problemCountiesBig<-colSums(adjMat)!=rowSums(adjMat)
asymCounties<-problemCountiesBig[problemCountiesBig]

asymAdjMat<-adjMat[names(asymCounties),names(asymCounties)]
cbind(colSums(asymAdjMat),rowSums(asymAdjMat))

adjacentCountiesFIPS[names(asymCounties)]
## wrkdat[names(asymCounties),]

## Just to check: These counties are not in some strange geographic situation.
###  with(wrkdat[wrkdat$state=='MN',],plot(CENTRDLAT,CENTRDLON,pch=1))
###  with(wrkdat[names(asymCounties),],points(CENTRDLAT,CENTRDLON,pch=19,col="red"))

## Fix the matrix itself: if any county is called adjacent to another county, make
## sure that this is reflected in both counties.

## Since the matrix is 0 when two counties are not adjacent and 1 when when
## they are adjacent, I can add the transpose of the matrix and turn missing 0's
## into 1's and existing 1's into 2's. So I then convert that submatrix into
## TRUE/FALSE and then back to 1/0. At least, this is my one line fix

adjMat[names(asymCounties),names(asymCounties)]<-as.numeric(
							    (adjMat[names(asymCounties),names(asymCounties)]+
							     t(adjMat[names(asymCounties),names(asymCounties)]))>0
							   )
stopifnot(isSymmetric(adjMat))

```

Make a matrix recording absolute differences in number of farmers between all pairs of counties.

```{r}
scalarDist<-function(var,scalefactor=1){
  ## Utility function to make n x n abs dist matrices
  ## Scalefactor helps us turn fractions into integers since nbpMatching needs
  ## integers
  outer(var,var,FUN=function(x,y){
	  as.integer(abs(x-y)*scalefactor)
							   })
}


## Create distance matrix for differences in number of farmers
numfarmers<-wrkdat$n
names(numfarmers)<-row.names(wrkdat)
sizeDist<-scalarDist(numfarmers)
## This will be an ingredient in the penalty that, we hope, forces pairs that are not adjacent but yet which are similar in terms of size and within the same state into pairs.
maxSizeDist<-max(as.vector(sizeDist))

```

```{r}

getStatePairs<-function(fips){
  adjD<-adjMat[fips,fips]
  sizeD<-sizeDist[fips,fips]
  penMat<- sizeD + adjD*maxSizeDist   ## add the maximum distance to any entry where two counties are adjacent
  if( (nrow(penMat) %% 2)!=0){
    penMat <- make.phantoms(penMat,1) ## delete one county where we have an odd number --- it will be the county least like the others, greatest distance on size and adjacency.
  }
  penMatD<-distancematrix(penMat)
  nbpm<-nonbimatch(penMatD)
  thepairs<-get.sets(nbpm$matches,remove.unpaired=TRUE)
  thepairsA<-as.character(thepairs) ## return as a character rather than a factor to make the state-by-state matching easier
  names(thepairsA)<-names(thepairs)
  return(thepairsA)
}

## Get the fips codes by state so that we can do state-by-state matching
fipsByState<-split(wrkdat$combifips,wrkdat$state)

## This is inelegant but intelligible and not slow because the matching problems are small in each state
pairsByState<-lapply(fipsByState,function(fips){ getStatePairs(fips)})

## An example of the output. Each county has a categorical variable (a character var) with a label for the pair it is in.
str(pairsByState[1:2])

## Because some states have an odd number of counties, one is excluded.
sum(sapply(pairsByState,length))
sapply(pairsByState,length)
## Compare to:
table(wrkdat$state)
sum(table(wrkdat$state))
nrow(wrkdat)

## Add the pair variable to the data
for(i in 1:length(pairsByState)){
  wrkdat[names(pairsByState[[i]]),"pm1"]<-pairsByState[[i]]
}

## For all non-missing values of the paired variable, we should have exactly 2 unique values.
stopifnot(all(table(wrkdat$pm1)==2))
```

How well did the matching do? Here are descriptives of the pairs in terms of
size and adjacency. We have one pair that is adjacent, so we will avoid it.
Also, since we did not ask the algorithmn to exclude bad matches, we have some
pairs that are very different in number of farmers. We will also avoid them
when choosing the one (or a few more) pairs for the design.

```{r}
pairedDat<-wrkdat[!is.na(wrkdat$pm1),]
pairDiffsN<-with(pairedDat,tapply(n,pm1,function(x){ abs(diff(x)) }) )
summary(pairDiffsN)
quantile(pairDiffsN,seq(0,1,.1))
```


How many pairs involve adjacent counties?

```{r}
pairDiffsAdj<-with(pairedDat,tapply(combifips,pm1,function(x){ all(as.vector(adjMat[x,x]==0)) }))
table(pairDiffsAdj)
```

## Choose best pairs within each state

We want to assign about 1/4 of the counties in each state to treatment. If this
seems like too much, we might decide to assign, say, 6 counties to treatment or
1/4 of the total counties, whichever is greater.

```{r choosepairs}

pairsByState<-sapply(split(pairedDat$pm1,pairedDat$state),unique) ## otherwise we'll have two entries for each pair

pairAvgN<-with(pairedDat,tapply(n,pm1,mean))


penalty1 <- function(x1,x2){
  ## here, x1 and x2 are the vectors of sizes (in numbers of farmers) in each county within a pair
  ifelse( (x1-x2) != 0,
    log(x1) + log(x2) - log(abs(x1 - x2)),
    log(x1) + log(x2) )
}


chooseBestPairs<-function(pm,numpairs){
  ## pm is a vector of pair-match names
  ## numpairs is the number of pairs to choose
  ## Given a vector of pair match labels, choose the best pair (closest match in size that is not adjacent with largest sample)

  ### First, exclude pairs where the two counties are adjacent (should be very rare given our pairing algorithm)
  okpairs<-names(pairDiffsN[pm][pairDiffsAdj[pm]]) ## exclude pairs where the two counties are adjacent
  sizeDiffs<-pairDiffsN[okpairs]
  avgN<-pairAvgN[okpairs]

  ### Second, sort the pairs by size difference as a proportion of average size of a county within in the pair.

  ### The idea here is that a 1000-900 pair is better than a 10-9 pair even
  ### though the size difference is 100 in the first place and only 1 in the second
  ### place.

  ## pairsInOrder<-cbind(sizeDiffs,avgN)[order(sizeDiffs,-1*avgN,decreasing=FALSE),] ## sort by diff in size and then by avgN
  ## pairsInOrder<-cbind(sizeDiffs,avgN,sizeDiffs/avgN)[order(sizeDiffs/avgN,-1*avgN,decreasing=FALSE),] ## sort by diff in size/avgN and then by avgN
  ## Rank the pairs by a combination of closeness in size and absolute size, breaking ties by size
  ordering<-order(penalty1(avgN-(sizeDiffs/2),avgN+(sizeDiffs/2)),avgN,decreasing=TRUE)
  pairsInOrder<-cbind(sizeDiffs,avgN,avgN-(sizeDiffs/2),avgN+(sizeDiffs/2),penalty1(avgN-(sizeDiffs/2),avgN+(sizeDiffs/2)))[ordering,] ## sort by diff in size/avgN and then by avgN

  ### Third, start choosing pairs in order from best to worst, requiring that already chosen counties not be adjacent to any other counties subsequantly chosen

  ## Decided not to focus on control-only adjacency since then the pairs are no
  ## longer fixed, pre-treatment features, but depend on the preceding ##
  ## randomizations. So, it would then be difficult to look for heterogeneous
  ## effects by type of pair (big counties, small counties, etc..)

  nAvail<-numpairs
  ##nFarmers<-0
  newpairsInOrder<-pairsInOrder
  res<-list() ## very inefficient but clear and clean versus res<-vector(mode="list",length=nrow(pairsInOrder))
  k<-0
  while(nAvail > 0 & nAvail <= numpairs){
    k<-k+1
    ## message(k)

    #### Choose the pair that is on top of the list, the best one according to our criteria above
    pair1<-strsplit(row.names(newpairsInOrder)[1],"-")[[1]]
    names(pair1)<-rep(row.names(newpairsInOrder)[1],2)
    ## pair1Z<-sample(c(0,1)) ## assign one county to treatment
    names(pair1)<-rep(row.names(newpairsInOrder)[1],2)
    ## res[[k]]<-data.frame(county=pair1,Z=pair1Z,pair=names(pair1))
    res[[k]]<-data.frame(county=pair1,pair=names(pair1),stringsAsFactors=FALSE)
    ##nFarmers1<-pairedDat[pair1[pair1Z==1],"n"]/2
    ##nFarmers<-nFarmers1+nFarmers
    ## message(nFarmers)

    #### Trim the pairsInOrder data to exclude the pairs already chosen and any pairs adjacent to the preceding pair.
    remainingPairs<-row.names(newpairsInOrder)[!(row.names(newpairsInOrder) %in% names(pair1)[1])]
    if(length(remainingPairs)==0){
      return(res)
    } else {
      ## Find pairs where at least one member is adjacent to one of the pair members and exclude that pair from consideration.
      ## adjM<-adjMat[pair1,##[pair1Z==0], unlist(strsplit(remainingPairs,"-"))]
      adjM<-adjMat[pair1,unlist(strsplit(remainingPairs,"-"))]
      adjCounties<-colnames(adjM)[colSums(adjM)>=1]
      adjCountiesLocs<-unique(sapply(c(pair1[1],adjCounties),function(cnty){ grep(cnty,row.names(newpairsInOrder)) }))
      includeCounties<-row.names(newpairsInOrder)[-c(adjCountiesLocs)]
      if(length(includeCounties)==0){ 
	return(res)
      } else {
	newpairsInOrder<-newpairsInOrder[includeCounties,,drop=FALSE]
	nAvail<-min(numpairs,nrow(newpairsInOrder))
	## message("Available pairs:",nAvail)
      }
    }
  }
}

targetPairsByState<-floor(sapply(pairsByState,length)/2) ## no more than 1/4 of the counties in a state in treatment

selectedPairs<-lapply(names(pairsByState),function(nm){
			pairList<-chooseBestPairs(pairsByState[[nm]],numpairs=targetPairsByState[[nm]])
			if(length(pairList)==0){
			  return(NA)
			} else {
			  results<-data.frame(do.call("rbind",pairList),stringsAsFactors=FALSE)
			  results$state<-nm
			  return(results)
			}
							   })
names(selectedPairs)<-names(pairsByState)


## How many counties did the algorithm return per state?
sapply(selectedPairs,function(x){ if(is.data.frame(x)){ return(nrow(x)) } else { return(NA) }})


prelimExperimentDat<-data.frame(do.call("rbind",selectedPairs),stringsAsFactors = FALSE)
row.names(prelimExperimentDat)<-prelimExperimentDat$county
prelimExperimentDat$n<-pairedDat[row.names(prelimExperimentDat),"n"]
str(prelimExperimentDat)
head(prelimExperimentDat)
summary(prelimExperimentDat$n) ## sizes of counties in the experiment
```

## Assign one county to possible treatment within each pair.

```{r}
set.seed(20150101)
prelimExperimentDat$Z<-unsplit(lapply(split(prelimExperimentDat$county,prelimExperimentDat$pair),function(x){ sample(c(0,1)) }),prelimExperimentDat$pair)
head(prelimExperimentDat)

stopifnot(all(tapply(prelimExperimentDat$Z,prelimExperimentDat$pair,sum)==1)) ## exactly 1 treated county per pair

```

## Preliminary Map

A map of our counties. Black are those counties assigned treatment. I think that this is very useful since we may want to go back and refine the algorithm a bit.

```{r map, fig.show='asis',fig.keep='high', fig.cap="Dark counties assigned to some treatment. Light counties assigned to no treatment"}
png(file="map.png",width=1920,height=1080)
par(oma=rep(0,4))
library(maps)
data(countyMapEnv)
data(usaMapEnv)
data(county.fips)
map('state',regions=states$State[states$Postal.Code %in% prelimExperimentDat$state],exact=FALSE,mar=c(1,1,1,1))
map("county",regions=county.fips$polyname[county.fips$fips %in% as.numeric(prelimExperimentDat$county)],add=TRUE)
map("county",regions=county.fips$polyname[county.fips$fips %in% as.numeric(prelimExperimentDat$county)[prelimExperimentDat$Z==1]],col="gray",fill=TRUE,add=TRUE)
dev.off()
```

![A Map of the counties.](map.png)

## Avoid cross-state treatment-to-control adjacency

The map makes us realize that we could contaminate the controls because of adjacency across state lines. This is rare, so we just dump those pairs where this happens --- keeping the pairs with more farmers.

```{r}

newAdjMat<-adjMat[prelimExperimentDat$county[prelimExperimentDat$Z==0],prelimExperimentDat$county[prelimExperimentDat$Z==1]]

## This many controls are adjacent to a treatment county
sum(rowSums(newAdjMat))

## Identify pairs with this problem. Some multiple adjacency

table(rowSums(newAdjMat))
table(colSums(newAdjMat))
ctrls<-rownames(newAdjMat)[rowSums(newAdjMat)>0]

crossStateAdjList<-apply(newAdjMat[ctrls,],1,function(x){ return(names(x[x>0,drop=FALSE]))})

dumpPairs<-lapply(names(crossStateAdjList),function(nm){
	       crossStateAdj<-prelimExperimentDat[c(nm,crossStateAdjList[[nm]]),]
	       dropPairs<-crossStateAdj$pair[order(crossStateAdj$n,decreasing=TRUE)][-1]
							   })
names(dumpPairs)<-names(crossStateAdjList)

experimentDat<-prelimExperimentDat[!(prelimExperimentDat$pair %in% unlist(dumpPairs)),]

nextAdjMat<-adjMat[experimentDat$county[experimentDat$Z==0],experimentDat$county[experimentDat$Z==1]]
stopifnot(sum(nextAdjMat)==0)

```

## Final Map


```{r map2, fig.show='asis',fig.keep='high', fig.cap="Design without cross-state contamination. Dark counties assigned to some treatment. Light counties assigned to no treatment"}
png(file="map2.png",width=1920,height=1080)
par(oma=rep(0,4))
library(maps)
data(countyMapEnv)
data(usaMapEnv)
data(county.fips)
map('state',regions=states$State[states$Postal.Code %in% experimentDat$state],exact=FALSE,mar=c(1,1,1,1))
map("county",regions=county.fips$polyname[county.fips$fips %in% as.numeric(experimentDat$county)],add=TRUE)
map("county",regions=county.fips$polyname[county.fips$fips %in% as.numeric(experimentDat$county)[experimentDat$Z==1]],col="gray",fill=TRUE,add=TRUE)
dev.off()
```

![A Map of the design with no cross-state contamination.](map2.png)



## Calculate the number of treatment assignments.

If we have 50% of farmers assigned to treatment within each potential-to-be-treated county, then we have the following total number of letters.

```{r}
proportionTreated <- 1/2

sum(experimentDat$n[experimentDat$Z==1] * proportionTreated)
```

More generally, here are some of the total numbers from different choices of proportion treated.

```{r}
totalTreated<-sapply(seq(.5,.75,.05),function(x){ sum(experimentDat$n[experimentDat$Z==1] * x) })
rbind(totalTreated,seq(.5,.75,.05))
```

Or we can just solve for the proportion to send out about 150,000 letters

```{r}

fn<-function(x){
  150000 - sum(experimentDat$n[experimentDat$Z==1] * x)
}

sol<-uniroot(fn,lower=.1,upper=.6)

sol$root

fn(sol$root)
sum(experimentDat$n[experimentDat$Z==1] * sol$root)

```

## Summary:
The treatment assignment at the county level was made within pairs. The pairs themselves are only made within state. So, in the analysis phase, we can use pair as the blocking variable without need for a separate state blocking variable.

## Save the file for use later

```{r}
write.csv(experimentDat,file="experimentDat.csv")
```

# References
